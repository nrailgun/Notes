1. 算法想不清楚可以画图（特别是树），实在不会做就纯暴力破解，不要留空。
2. 可能有 linux syscall 相关题目，熟悉 api 很简单（比如考察 `pthread_cond`）。如果遇到不熟的 syscall，YY 函数一个来代替，虽然扣分，好过空过。
3. 如果问题较难实在想不出来，面试官穷追不舍，可以考虑要求跳过问题。否则气氛尴尬减印象分，还导致后续没有时间讲自己项目亮点。

---

我叫吴俊宇，中山大学 18 届计科研究生，目前在网易游戏现金流产品《大话西游手游》任职后台开发工程师。主要负责业务功能开发，和部分分布式组件的维护迭代，工作 2 年，0 事故，升职两次，加薪两次。工作中常用语言为 lua，golang，c++ 也有使用。对 raft 等常见分布式一致性算法有一定了解，简单使用过 docker 和 k8s。

---

任务：加速单层 embedding 特征计算 $y = Wx$。Leader 算法是：对 $x$ 进行 k-means 聚类，再对聚类中心 $C_x$ 预结算 $y = WC_x$；在输入 $x$ 时找到最近的聚类中心，直接返回缓存结果，借此加速。实际测试后发现该算法无效。在阅读了大量相关文档后，分析其中问题：

1. K-Means 并不比矩阵乘法快，
2. 没有利用 SIMD 指令，
3. 通过阅读 BLAS 相关文档，发现 CPU cache 在矩阵运算中的重要地位。

根据以上 3 点，结合 $x$ 为 soft-sign 算子 输出的性质 $0 \le x_i \le 1$，我进行了以下改动：

1. 离散下采样比 k-means 聚类快很多，所以穷举所有 $x$ 进行预结算，下采样粒度为 $b$，
2. 为了让预结算结果尽量小，容易放入 L2 cache，我将 $x$ 切割为长度 $d$ 的小段，单独计算后通过 SIMD 进行快速累加。

在 c++ demo 中取得了成效，随后在其 c++ 框架 lego 中实现后发现效果与 demo 有出入。经过数日分析后意识到可能是 g++ 生成了错误的代码，查阅汇编代码后发现确实 g++ 生成了一段无用的冗余代码（O2 优化下）。改用汇编编写后解决问题，SSE 对比加速 15%，AVX 对比加速 80%。

---

需求：从 100 台备选服务器中，根据在线人数以及少量特殊规则排序，选中 10 台服务器，用于伺服跨服玩法。

CSM 模块负责该功能，在一次偶然的事故中，leader 要求我查看 CSM 是否有 bug。CSM 存在 3 个 bug：

1. 主从同步分配结果存在明显 bug，传输后数据拷贝错位置，直接丢失，
2. 不等待的同步策略，可能在响应请求后 crash 丢失，从 slave 取会重新计算，取得一个不同结果，导致玩法出现问题，
3. 关键问题在于，master fail reboot 之后，无法正确从 slave 同步回分配结果，客户端也无法明确哪个是 master。一致性存储的实现是很困难的。

简单而言两个问题：容灾和一致性。我在查阅了 paxos、raft 等论文，并调研了相关工具后，初步选定了 etcd 作为解决方案，主动向 leader 请求修复该问题。

1. 分布式锁并不能保证 atomic，如果途中 crash，数据会出于不合法状态。此外，其他节点接手锁也需要等待。
2. 而 etcd 的 transaction 仅支持 test / get / set，不支持递归和 inc，不具有支持分配算法的可计算性。

最终的方案是使用 stm，在数据冲突不严重的情况下，可以利用编程语言进行灵活的计算。此时又面临另外两个问题：

1. lua 没有 official driver，自己用 lua 写 stm 很比较繁琐，而开源代码暂时没有稳妥库可用，
2. 备选服务器在 etcd 上注册自己，可能和分配过程形成读写冲突，导致乐观锁不停重试。

最终的解决方案是，使用 golang 编写两个无状态的 ambassador，使用 official driver 去做 stm，同时这两个 ambassador 会通过内存锁将所有的请求进行排队，抑制 etcd 的并发量，减少读写冲突。如此，解决了容灾和一致性的问题。

---

Q：为什么 all ack 是行不通的策略？

A：因为退化成了类 2pc 的策略，availability 失去了（这还不考虑 mongodb 的 bug）。而且 2pc 是两段式的，有一个确认过程，mongodb 写上去就写上去了。

Q：那么 majority ack 行不行得通呢？

A：即使不考虑成员变化这种复杂的情况，单纯的 majority ack 依旧行不通。缺乏 *more-up-to-date* 机制的支持，选举可能选中 stale 的节点。例如 ABC 三个节点，A 是 master。

1. A、B sync $L_1$，回报客户端 commit。
2. A 和 B、C 网络中断，但是依旧 accept $L_2$， $L_3$，和 $L_4$，但是未能 commit。
3. B、C 重新选举，单纯论 log 长度，B 单选。
4. B 和 C commit 了 $L_5$。
5. 此时 A 突然重连成功，B 突然 fail，A 会当选，覆盖 C 的 committed log。

Q：MongoDB 不行吗？

A：MongoDB 的存储引擎 MMAPv1 并不保证 wal 写入磁盘，所以即使使用了 majority ack + more up to date，依旧 lost acked data。

Q：MySQL 不行吗？

A：MySQL 不是特别了解，公司不用 MySQL，不了解就不踩坑了。但是 MySQL 默认策略也是 async replication。

MongoDB 和 MySQL 的卖点并不是一致性，他们的文档也对于一致性没有明确的保证，而且实现中的各种细节也未可知，试图用 MySQL / MongoDB 的某些参数去模拟 raft 算法是非常危险的举动。如果出事了，领导一问，我操你真是个小机灵鬼有现成方案不用，自己发明破轮子。背锅侠稳吃绩效 C。而 Etcd 的提供了明确保证，而且广受市场验证，没必要自己强行作死用 MongoDB 或者 MySQL 去做一致性存储。

---

小组作业：

- 使用 meta programming 实现 message 自动序列化、反序列化：
  - 比 json 节约带宽，
  - 只需要定义 python class，非常直观。
- 使用 meta programming 仿制 django sql orm。只需要定义 python class，无需编写 sql 语句，省事而且防止注入。
- 游戏逻辑框架设计与实现
  - 在团队中推广 ecs 范式，并实现了框架。
  - 实践中我们发现这确实大大便利了我们的编程。缺点是逻辑与先后顺序有时候比较模糊，但对于 tps 核心玩法，这影响很小。
- 游戏内 npc 的 pathfinding 是核心工作，bonus 大头。
  - 基本思路是将地图切分为 3 角或者 4 角网格，网格内可以随意直线移动，不同网格间使用 A* 寻路。4 角网格容易切分，容易移动，但是会导致一个僵硬的路径（永远 90 度转动）。如果高密度切分，问题会缓解，但是会急剧消耗 CPU。3 角网格难切分，难实现，但是视觉效果好，CPU 消耗低。我选择了后者。三角网格寻路是工程实践，没有论文可参考。我参考了 3A 级开源方案 detour，约 100+k 行 c++ 代码，核心代码 20k。
  - 三大瓶颈：物理模拟，寻路，渲染。一秒钟要计算 60 帧，平均只有 0.016s。有 100+ NPC，数千网格。
  - 优化：
    - 三角路径寻路路径点呈 z 型，需要通过 simple stupid funnel algorithm 进行路径拉直，beat 90%。
    - 每帧不要一次性计算全部，只计算一部分 NPC。减轻开销同时形成一种 NPC 动作不同步的错落感（这很重要！）。
    - 玩家速度其实不快，如果移动但没有离开原网格，不要重新计算路径。
    - 使用最近的边求所在网格，而不是最近的中心，否则 npc 会“闪现”。
    - 不要使用网格中心点距离做代价估算，使用穿入边穿出边中点距离估算。解决 99% 的狭长三角导致的怪异路径。
    - 碰撞用最简单的 circle-circle 方案，如果 npc pairwise 计算碰撞会导致 cpu 爆炸，所以我做了一个简单的 grid aoi，只将处在同一个矩形区域内的 npc 做碰撞检测。
  - 说起来都很简单，但是你要在两个星期内对接客户端和看这么多算法和肮脏 trick 的代码+实现+调优，其实有难度。

---

Debug 先定位，top 可以，其实一般用公司内网的监控，一条龙看 cpu、memory、disk 和 networking 等。

如果是一般的业务错误，看 log 一般就 ok 了。

如果是 c++ coredump 了，就 gdb 看 core，bt 看栈，一般能定位。上次定位了一个 `luaL_error` 导致的错误。

如果是死锁，统计下线程多数阻塞在哪一行，一般围绕死锁 4 条件分析一阵子也能分析出来。

如果是分布式协议出一致性错误这种就十分十分头疼了，基本上只能看 log，对着论文分析条件，只能看直觉，相当困难。

---

Q：如何优化？

A：先看 ELK 之类的分析日志，根据吞吐量找出系统中的瓶颈。可以考虑用 top / iotop / strace 先定性问题（io / too many syscalls / too many fsync / stupid impl），然后用 gprof 之类的工具进行分析，找到耗时较多部分。

如果只是 stupid impl 就很容易解决。有些算法本身的问题往往比较难解决，只能根据实际情况进行优化（比如我在百度做的）。某些问题可能有些套路，比如是否 hash 分布不合理，有些问题可以用 lazy approx solution，有些问题可以用视觉骗术。

如果是 io 量有问题：

1. 先排除掉是否有小文件，不合理频繁 rw 问题，随机寻道问题，
2. 考虑尽量客户端拦截无用请求，限制访问频率，
3. 动静分离，CDN，
4. 控制放到数据层的请求量，避免压垮数据层，系统繁忙直接拒绝请求（使用 MQ，自发拉取，控制处理量），
5. 使用缓存，但要注意缓存穿透（反复查找不存在对象）、击穿（短时间内重复查找同一对象）、雪崩问题（缓存同时过期）。

---

常见站点架构层次：

1. 浏览器
2. 站点：访问后端，拼 HTML 页面。
3. 服务：向上游屏蔽业务细节。
4. 数据：数据存储，如 MySQL。

优化方向：

1. 尽量将请求拦截在系统上游：请求量过大会压倒数据层，数据读写锁冲突严重，并发高响应慢。
2. 充分利用缓存（读多写少）。

优化细节：

1. 客户端拦截
   1. 良好 UI 设计
   2. 客户端限制（禁止重复点击）
2. 站点层拦截
   1. 限制访问频率，
   2. 缓存返回页面。
   3. 动静分离，CDN。
3. 服务层
   1. 控制放到数据层的请求量，避免压垮数据层。
   2. 使用 MQ，自发拉取，控制处理量。
   3. 使用缓存，但要注意缓存穿透（反复查找不存在对象）、击穿（短时间内重复查找同一对象）、雪崩问题（缓存同时过期）。