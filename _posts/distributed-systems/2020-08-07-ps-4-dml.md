---
layout: post
mathjax: true
title: "Parameter Server for Distributed Machine Learning"
categories: distributed-systems
date: 2020-08-07 00:00:00
---

# Parameter Server for Distributed Machine Learning

论文关注点是，使用 paramter server 去做分布式最优化：

1. Client 负责部分数据和 workload，server 负责存取全局共享的参数（sharding & replication）。
2. 通过 DHT 来允许运行期扩展。
3. 设计上支持 matrix / vector，比 kv store 的实现更易用（*redis 那样加一些数据结构？*）。

## Consistency

Maximal delayed time：最多允许部分 push 落后 $\tau$ 个 iter，允许 client 所见的 parameter 有限的不同步。

User-defined filters：ps 可以 filter 掉一些 diff，比如 $|w_k - w_k^{(\mathrm{synced})}| > \Delta$。

## Reliability

使用 DHT for assignment，使用 paxos 存储 key segment $\to$ node 的映射。为了 fault tolerance，每个 key segment 会 dup 到 clockwise 的 $k$ 个邻居中。添加 virtual node 来增强 load-balancing。

## Theoreticall analysis

论文通过 *proximal splittings methods in signal processing* 的 proximal gradient methods 对于收敛保证进行理论证明。DL 的优化与收敛问题本来就是个未解之谜，这个证明总感觉有点不可思议。暂时没有细看，如果有必要的话可以再另行深入了解。

# Large scale distributed deep networks

Google Jeff Dean 的论文，DistBrief，TensorFlow 的前身。Ps4dml 提到他们的工作与该论文非常相似，因此过了一下。

目标很简单，多机器的 DL（数据并行与模型并行）。模型并行的问题问题在于：常见的优化算法 SGD 是 inherently sequential，因此 Jeff Dean 提出了改进版的 D-SGD（下图）。

<img src="https://www.cs.dartmouth.edu/~lorenzo/teaching/cs174/Archive/Winter2013/Projects/FinalReportWriteup/piotr.teterwak.14/Illustration.png" alt="D-SGD" style="zoom:50%;" />

数据通过 data shard 并行，拥有多个 model replicas；parameter servers 拥有多个实例，每个 ps 拥有一部分参数。最简单的做法是，每次计算之前，都 fetch 一次参数，然后 push 一次 gradient。为了减少网络通信，也可以每 $N_f$ iter 取一次参数，$N_p$ iter push 一次 gradient。***显然，参数的一致性多多少少有些 out-of-date，但是实践证明，对于 DL 并不造成太多问题***（说不定还有好处呢...）。

L-BFGS 深度学习应用不多（？），就暂时不深入了。

# Consistent Hashing and Random Trees

这是一致性哈希的论文，由于 PS4DML 的 DHT *simple load balancing for distributed hash tables* 的论文基于此展开，因此顺路看下。

## Random Tree

假定对于所有的机器，所有的 cache 机器已知。

1. 当浏览器请求页面，从 $C$ 个节点的 $d$-nary abstract tree 中随机选取一个 leave to root path。$C$ 是 cache 机器集合 $\mathcal C$ 的 size。使用哈希函数 $h$ 将 path 中的节点映射到一个 cache 机器，尝试请求页面。

2. 当 cache 机器收到请求，如果 page 存在于 cache 中，那么直接返回；否则增加页面的计数，并按照 browser 指定的 path，向上一层的节点请求液面。如果页面的计数到达了一个阈值 $q$，cache 机器会向 page 服务器要求伺服一份 cache。

为什么这个方法可以避免热点呢？因为：

1. 显然，page 不能都集中在一个 server 上，否则肯定很容易 swamped。通过将 page 分配到逐个 server 中能比较好地解决这个问题。
2. 但是即使一个机器只有一个页面，如果这个页面非常受到欢迎，那么单个 cache 仍然很容易被 swamped。所以我们需要一个树形的结构。如果页面不受欢迎，只会在靠近根部节点才有足够多的 req，才会进行缓存（更少的缓存服务器节约成本）。如果页面收到欢迎，那么叶子处也会有非常多请求，那么就会有非常多的叶子处的缓存（更多 cache 服务器来分摊压力）。
3. 直观上理解，靠近根部的 cache 机器伺服更多的页面，即使页面并不是热点，也容易被 swamped。因此，对每一个页面，abstract tree 都是随机生成的不同的树，这确保底部节点不会伺服太多页面，因此不太会被 swamp。树的 root 是特殊的，不是 cache 而是 page server。

如此，random tree 方法在 $\Theta (\log C)$ 的 delay 的代价下避免了 cache 被 swamp。

## Consistent Hashing

论文的写法比较抽象：给定随机函数 $r_B$ 和 $r_I$，$f_V(i)$ 被定义为 $b \in V$ 使得 $|r_B(b) - r_I(i)|$ 最小化。

其实际上的意思是，将桶 $b$ 通过哈希 $r_B$ 分布到一个区间（例如 $[0, 2^{31-1}]$），$i$ 通过哈希 $r_I$ 进行映射，并分配到其附近的桶中。

<img src="https://images2015.cnblogs.com/blog/498077/201608/498077-20160822172901526-169091807.png" alt="一致性哈希图解" style="zoom:50%;" />

如果增加、减少 bucket，那么只影响附近两个 bucket。此外，为了减少数据倾斜，一般每个节点都会插入一定量的虚拟节点。

# Simple Load Balancing for Distributed Hash Tables

DHT 的论文。阅读中....
