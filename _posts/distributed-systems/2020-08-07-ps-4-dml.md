---
layout: post
mathjax: true
title: "Parameter Server for Distributed Machine Learning"
categories: distributed-systems
date: 2020-08-07 00:00:00
---

# Parameter Server for Distributed Machine Learning

论文关注点是，使用 paramter server 去做分布式最优化：

1. Client 负责部分数据和 workload（*data parallelism instead of model parallelism*），server 负责存取全局共享的参数（sharding & replication）。
2. 通过 DHT 来允许运行期扩展。
3. 设计上支持 matrix / vector，比 kv store 的实现更易用（*redis 那样加一些数据结构？*）。

## Consistency

Maximal delayed time：最多允许部分 push 落后 $\tau$ 个 iter，允许 client 所见的 parameter 有限的不同步。

User-defined filters：ps 可以 filter 掉一些 diff，比如 $|w_k - w_k^{(\mathrm{synced})}| > \Delta$。

## Reliability

使用 DHT for assignment，使用 paxos 存储 key segment $\to$ node 的映射。为了 fault tolerance，每个 key segment 会 dup 到 clockwise 的 $k$ 个邻居中。添加 virtual node 来增强 load-balancing。

## Theoreticall analysis

论文通过 *proximal splittings methods in signal processing* 的 proximal gradient methods 对于收敛保证进行理论证明。李沐做的是 DL，DL 的优化与收敛问题本来就是个未解之谜，这个证明总感觉有点不可思议。暂时没有细看，如果有必要的话可以再另行深入了解。

# Simple Load Balancing for Distributed Hash Tables



# Large scale distributed deep networks





























# Qs

1. AI Lab 是否提供了类似的分布式 DL 框架？AI Lab 比我们更贴近 DL，我们的 advantage 是什么？更加贴近业务，更多的数据？