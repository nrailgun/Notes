---
layout: post
mathjax: true
title: "Parameter Server for Distributed Machine Learning"
categories: distributed-systems
date: 2020-08-07 00:00:00
---

# Parameter Server for Distributed Machine Learning

论文关注点是，使用 paramter server 去做分布式最优化：client 负责部分数据和 workload（*个人理解：数据并行 instead of 流水线*），server 负责存取全局共享的参数。Google 的 *Large Scale Distributed Deep Networks* 使用了类似的想法去处理深度学习。































# Large scale distributed deep networks

# Simple Load Balancing for Distributed Hash Tables

# Qs

1. AI Lab 是否提供了类似的分布式 DL 框架？AI Lab 比我们更贴近 DL，我们的 advantage 是什么？更加贴近业务，更多的数据？