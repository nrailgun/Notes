---
layout: post
mathjax: true
title: "Parameter Server for Distributed Machine Learning"
categories: distributed-systems
date: 2020-08-07 00:00:00
---

# Parameter Server for Distributed Machine Learning

论文关注点是，使用 paramter server 去做分布式最优化：

1. Client 负责部分数据和 workload，server 负责存取全局共享的参数（sharding & replication）。
2. 通过 DHT 来允许运行期扩展。
3. 设计上支持 matrix / vector，比 kv store 的实现更易用（*redis 那样加一些数据结构？*）。

## Consistency

Maximal delayed time：最多允许部分 push 落后 $\tau$ 个 iter，允许 client 所见的 parameter 有限的不同步。

User-defined filters：ps 可以 filter 掉一些 diff，比如 $|w_k - w_k^{(\mathrm{synced})}| > \Delta$。

## Reliability

使用 DHT for assignment，使用 paxos 存储 key segment $\to$ node 的映射。为了 fault tolerance，每个 key segment 会 dup 到 clockwise 的 $k$ 个邻居中。添加 virtual node 来增强 load-balancing。

## Theoreticall analysis

论文通过 *proximal splittings methods in signal processing* 的 proximal gradient methods 对于收敛保证进行理论证明。DL 的优化与收敛问题本来就是个未解之谜，这个证明总感觉有点不可思议。暂时没有细看，如果有必要的话可以再另行深入了解。

# Large scale distributed deep networks

Google Jeff Dean 的论文，DistBrief，TensorFlow 的前身。Ps4dml 提到他们的工作与该论文非常相似，因此过了一下。

目标很简单，多机器的 DL（数据并行与模型并行）。模型并行的问题问题在于：常见的优化算法 SGD 是 inherently sequential，因此 Jeff Dean 提出了改进版的 D-SGD（下图）。

<img src="https://www.cs.dartmouth.edu/~lorenzo/teaching/cs174/Archive/Winter2013/Projects/FinalReportWriteup/piotr.teterwak.14/Illustration.png" alt="D-SGD" style="zoom:50%;" />

数据通过 data shard 并行，拥有多个 model replicas；parameter servers 拥有多个实例，每个 ps 拥有一部分参数。最简单的做法是，每次计算之前，都 fetch 一次参数，然后 push 一次 gradient。为了减少网络通信，也可以每 $N_f$ iter 取一次参数，$N_p$ iter push 一次 gradient。***显然，参数的一致性多多少少有些 out-of-date，但是实践证明，对于 DL 并不造成太多问题***（说不定还有好处呢...）。

