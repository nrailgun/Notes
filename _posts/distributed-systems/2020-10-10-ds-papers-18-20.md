---
layout: post
mathjax: true
title: "分布式系统论文选读与摘要(2018-2020)"
categories: distributed-systems
date: 2020-10-10 00:00:00
---

# OSDI 2018

## Ray: A Distributed Framework for Emerging AI Applications

支持 RL 任务，之前都是需要手写 end-to-end 的。论文没有细看。

## Gandiva: Introspective Cluster Scheduling for Deep Learning

讨论了 gpu scheduling，大的 task 分时，小的 task 塞在同一个 gpu。

## Splinter: Bare-Metal Extensions for Multi-Tenant Low-Latency Storage

允许 client 上传一个 rust 的 subset 来执行（避免多次 rpc round-trip），但是同时可以避免安全问题。

## Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis

感觉应该发在 dl 的会议，就是一个多 step 逐渐 super-resolution 的神经网络。如果用户带宽都不容乐观，怎么会有强力 gpu realtime super-resolution video 呢？

# SOSP 2019

## PipeDream: Generalized Pipeline Parallelism for DNN Training

bp apply gradient 完成之前可以先用 prev version 的 parameters 计算 fp，虽然 last mini-batch 的可能看不到，由于训练速度 lr 较低，论文认为完全不影响收敛。

大部份现代 framework 都有类似 wfbp 的机制，不过也无法避免等待 gradient 通信。

# OSDI 2020

*Taiji: Managing Global User Traffic for Large-Scale Internet Services at the Edge* 讨论了动态路由，有需要可以看下。

*Semeru: A Memory-Disaggregated Managed Runtime* 单个节点只提供非常非常单个的资源（cpu、内存、gpu），通过高速 rdma 互联。

*Overload Control for µs-scale RPCs with Breakwater* 关于过载拒绝，值得关注。

*Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks*，*Beyond Data and Model Parallelism for Deep Neural Networks*，*One Weird Trick for Parallelizing Convolutional Neural Networks* 讨论了 cnn 的 model 并行。

*TASO: Optimizing Deep Learning Computation with Automated Generation of Graph Substitutions* 有十分神奇的 graph optimizing，是否可用？

# Others

## Adaptive Batching for Replicated Servers

batching（合批）要么 count based 或者 time based，两者都需要人工设置阈值：

- count based 可能等不到 batch size 个 req，一直阻塞。
- time based 如果阈值太大会导致 latency 过大，如果太小则约等于没有 batching。

time adaptive batching 是指用很小的时间阈值 $\mu$，如果 $\mu$ 时间内有另一个 req 到达，则继续排队，否则 batch 发送。

## Detection of Mutual Inconsistency in Distributed Systems

经典 work：version vector。

## Concise Version Vectors in WinFS

## P2P Replica Synchronization with Vector Sets

## Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis

介绍了各种神经网络计算分布式与并行相关的工作。

## Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters

论文指出 tf 32x 机器只能有 20x 提升。

wait-free backpropagation (WFBP)：bp 操作是反向的，communication 操作可以在 bp 已经完成的后部提前开始同步进行。tf iteration step 之间存在 global barrier，此外取参数也是在 fp 过程的，其实可以放在 bp 过程。

hybrid communication：根据 param shape 决定是 ps 或 p2p 传播。不过论文的 p2p 传播是 sfb，而不是常见的 all-reduce。

